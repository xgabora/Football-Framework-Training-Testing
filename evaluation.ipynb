{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "This is the example showcase of testing the prediction framework proposed in the thesis. Results may vary from the reported benchmark results due to the random nature of some used principles."
   ],
   "id": "8b8e1730b074a153"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A. Evaluating Pre-match model",
   "id": "a51a7131ac274880"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data loading\n",
    "\n",
    "Effortlessly view, navigate, sort, and filter data. Create charts and access essential data insights, including descriptive statistics and missing values â€“ all without writing a single line of code."
   ],
   "id": "2431bdce38f1eaea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T21:18:15.532992Z",
     "start_time": "2025-05-10T21:18:15.459437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "DATA_FILE = 'training/football_data.csv'\n",
    "MODEL_FULL_PATH = 'models/pre_match/full_features.keras'\n",
    "MODEL_REDUCED_PATH = 'models/pre_match/reduced_features.keras'\n",
    "\n",
    "TARGET_COL = 'FTResult'\n",
    "TARGET_MAP = {'H': 0, 'D': 1, 'A': 2}\n",
    "INV_TARGET_MAP = {v: k for k, v in TARGET_MAP.items()}\n",
    "\n",
    "# --- Define feature sets ---\n",
    "RAW_INPUT_FEATURES_FROM_CSV = [\n",
    "    'Division', 'MatchDate', 'HomeTeam', 'AwayTeam', 'FTResult', \n",
    "    'HomeElo', 'AwayElo', \n",
    "    'Form3Home', 'Form5Home', 'Form3Away', 'Form5Away', \n",
    "    'OddHome', 'OddDraw', 'OddAway',\n",
    "    'Over25', 'Under25', 'MaxOver25', 'MaxUnder25', \n",
    "    'HandiSize', 'HandiHome', 'HandiAway'\n",
    "\n",
    "]\n",
    "FEATURES_TO_LOAD_FROM_CSV = list(set(RAW_INPUT_FEATURES_FROM_CSV))\n",
    "\n",
    "\n",
    "NUMERICAL_FEATURES_FULL_MODEL = [ \n",
    "    'HomeElo', 'AwayElo', 'EloDiff', 'Form3Home', 'Form5Home', 'Form3Away', 'Form5Away',\n",
    "    'Form3Diff', 'Form5Diff', 'OddHome', 'OddDraw', 'OddAway', 'Over25', 'Under25',\n",
    "    'MaxOver25', 'MaxUnder25', 'HandiSize', 'HandiHome', 'HandiAway', 'Year', 'Month'\n",
    "]\n",
    "NUMERICAL_FEATURES_REDUCED_MODEL = [ \n",
    "    'Form3Home', 'Form5Home', 'Form3Away', 'Form5Away', 'Form3Diff', 'Form5Diff',\n",
    "    'OddHome', 'OddDraw', 'OddAway', 'Year', 'Month'\n",
    "]\n",
    "CATEGORICAL_LOW_CARDINALITY_MODEL = ['DayOfWeek'] \n",
    "CATEGORICAL_HIGH_CARDINALITY_MODEL = ['HomeTeam', 'AwayTeam', 'Division'] \n",
    "\n",
    "FULL_MODEL_RAW_DATA_REQUIREMENTS = [\n",
    "    'HomeElo', 'AwayElo', 'Over25', 'Under25', 'MaxOver25', 'MaxUnder25',\n",
    "    'HandiSize', 'HandiHome', 'HandiAway'\n",
    "]\n",
    "ODDS_COLS = ['OddHome', 'OddDraw', 'OddAway']\n",
    "\n",
    "\n",
    "EVALUATION_SUBSET_SIZE = 500\n",
    "NN_BATCH_SIZE = 256\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ],
   "id": "56ff4faa0a8ac151",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data cleaning, pre-processing and feature engineering\n",
    "\n",
    "Create graphs and visualizations that match your chosen color scheme."
   ],
   "id": "36f201f4f525f089"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T21:18:30.203326Z",
     "start_time": "2025-05-10T21:18:27.715491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_clean_data(data_file, csv_cols_to_load, target_col, odds_cols):\n",
    "    \"\"\"Loads specified columns, cleans dates/target, handles odds.\"\"\"\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"Data file not found: {data_file}\")\n",
    "\n",
    "    df = pd.read_csv(data_file, usecols=csv_cols_to_load, low_memory=False)\n",
    "\n",
    "    df['MatchDateTime'] = pd.to_datetime(df['MatchDate'], errors='coerce')\n",
    "    df.dropna(subset=['MatchDateTime'], inplace=True)\n",
    "    df.sort_values(by='MatchDateTime', ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df = df[df[target_col].isin(TARGET_MAP.keys())]\n",
    "    if df.empty: raise ValueError(\"DataFrame empty after filtering for valid target values.\")\n",
    "\n",
    "    for col in odds_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.loc[df[col] <= 0, col] = 1e6\n",
    "    df.dropna(subset=odds_cols, inplace=True)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty after cleaning odds.\")\n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Engineers time-based and differential features.\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    df_eng['Year'] = df_eng['MatchDateTime'].dt.year\n",
    "    df_eng['Month'] = df_eng['MatchDateTime'].dt.month\n",
    "    df_eng['DayOfWeek'] = df_eng['MatchDateTime'].dt.dayofweek.astype(str)\n",
    "\n",
    "    cols_for_diff_calculation = {\n",
    "        'Form3Home': 0, 'Form3Away': 0, 'Form5Home': 0, 'Form5Away': 0,\n",
    "        'HomeElo': 1600, 'AwayElo': 1500 \n",
    "    }\n",
    "    for col, default_val in cols_for_diff_calculation.items():\n",
    "        if col in df_eng.columns:\n",
    "            df_eng[col] = pd.to_numeric(df_eng[col], errors='coerce')\n",
    "            if df_eng[col].isnull().any():\n",
    "                df_eng[col].fillna(df_eng[col].median(), inplace=True)\n",
    "        else: \n",
    "            df_eng[col] = default_val\n",
    "\n",
    "    df_eng['Form3Diff'] = df_eng['Form3Home'] - df_eng['Form3Away']\n",
    "    df_eng['Form5Diff'] = df_eng['Form5Home'] - df_eng['Form5Away']\n",
    "    df_eng['EloDiff'] = df_eng['HomeElo'] - df_eng['AwayElo']\n",
    "    return df_eng\n",
    "\n",
    "def fit_preprocessors(df_engineered, num_feat_full_model, num_feat_reduced_model,\n",
    "                      cat_low_feat_model, cat_high_feat_model):\n",
    "    \"\"\"Fits all preprocessors based on the full engineered dataset.\"\"\"\n",
    "    preprocessors = {}\n",
    "\n",
    "    available_num_full = [col for col in num_feat_full_model if col in df_engineered.columns]\n",
    "    available_num_reduced = [col for col in num_feat_reduced_model if col in df_engineered.columns]\n",
    "    available_cat_low = [col for col in cat_low_feat_model if col in df_engineered.columns]\n",
    "    available_cat_high = [col for col in cat_high_feat_model if col in df_engineered.columns]\n",
    "\n",
    "    # Imputers\n",
    "    if available_num_full:\n",
    "        preprocessors['num_imputer_full'] = SimpleImputer(strategy='median').fit(df_engineered[available_num_full])\n",
    "    if available_num_reduced:\n",
    "        preprocessors['num_imputer_reduced'] = SimpleImputer(strategy='median').fit(df_engineered[available_num_reduced])\n",
    "    if available_cat_low:\n",
    "        preprocessors['cat_low_imputer'] = SimpleImputer(strategy='most_frequent').fit(df_engineered[available_cat_low])\n",
    "    if available_cat_high:\n",
    "        preprocessors['cat_high_imputer'] = SimpleImputer(strategy='most_frequent').fit(df_engineered[available_cat_high])\n",
    "\n",
    "    df_imputed = df_engineered.copy()\n",
    "    if 'num_imputer_full' in preprocessors and available_num_full:\n",
    "        df_imputed[available_num_full] = preprocessors['num_imputer_full'].transform(df_imputed[available_num_full])\n",
    "    if 'num_imputer_reduced' in preprocessors and available_num_reduced:\n",
    "        df_imputed[available_num_reduced] = preprocessors['num_imputer_reduced'].transform(df_imputed[available_num_reduced])\n",
    "    if 'cat_low_imputer' in preprocessors and available_cat_low:\n",
    "        df_imputed[available_cat_low] = preprocessors['cat_low_imputer'].transform(df_imputed[available_cat_low])\n",
    "    if 'cat_high_imputer' in preprocessors and available_cat_high:\n",
    "        df_imputed[available_cat_high] = preprocessors['cat_high_imputer'].transform(df_imputed[available_cat_high])\n",
    "\n",
    "    # Scalers\n",
    "    if 'num_imputer_full' in preprocessors and available_num_full:\n",
    "        preprocessors['scaler_full'] = StandardScaler().fit(df_imputed[available_num_full])\n",
    "    if 'num_imputer_reduced' in preprocessors and available_num_reduced:\n",
    "        preprocessors['scaler_reduced'] = StandardScaler().fit(df_imputed[available_num_reduced])\n",
    "\n",
    "    if 'cat_low_imputer' in preprocessors and available_cat_low:\n",
    "        preprocessors['ohe_low'] = OneHotEncoder(sparse_output=False, handle_unknown='ignore').fit(df_imputed[available_cat_low])\n",
    "\n",
    "    # Label Encoders\n",
    "    label_encoders = {}\n",
    "    vocab_sizes = {}\n",
    "    unk_token = '<UNK>' \n",
    "\n",
    "    team_cols_for_le = [col for col in ['HomeTeam', 'AwayTeam'] if col in available_cat_high]\n",
    "    if team_cols_for_le and 'cat_high_imputer' in preprocessors:\n",
    "        all_teams = pd.concat([df_imputed[col] for col in team_cols_for_le]).astype(str).unique()\n",
    "        le_team = LabelEncoder().fit(np.append(all_teams, unk_token))\n",
    "        label_encoders['Team'] = le_team\n",
    "        vocab_sizes['Team'] = len(le_team.classes_)\n",
    "\n",
    "    if 'Division' in available_cat_high and 'cat_high_imputer' in preprocessors:\n",
    "        all_divisions = df_imputed['Division'].astype(str).unique()\n",
    "        le_div = LabelEncoder().fit(np.append(all_divisions, unk_token))\n",
    "        label_encoders['Division'] = le_div\n",
    "        vocab_sizes['Division'] = len(le_div.classes_)\n",
    "\n",
    "    preprocessors['label_encoders'] = label_encoders\n",
    "    preprocessors['vocab_sizes'] = vocab_sizes\n",
    "\n",
    "    preprocessors['fitted_cols_num_full'] = available_num_full\n",
    "    preprocessors['fitted_cols_num_reduced'] = available_num_reduced\n",
    "    preprocessors['fitted_cols_cat_low'] = available_cat_low\n",
    "    preprocessors['fitted_cols_cat_high'] = available_cat_high\n",
    "\n",
    "    return preprocessors\n",
    "\n",
    "def transform_data_for_keras(df_slice, preprocessors,\n",
    "                             num_feat_model_list, cat_low_feat_model_list, cat_high_feat_model_list,\n",
    "                             model_type_flag, loaded_model_ref):\n",
    "    \"\"\"Transforms a slice of data using fitted preprocessors for Keras model input.\"\"\"\n",
    "    df_proc = df_slice.copy()\n",
    "\n",
    "    num_imputer = preprocessors.get(f'num_imputer_{model_type_flag}')\n",
    "    scaler = preprocessors.get(f'scaler_{model_type_flag}')\n",
    "    cat_low_imputer = preprocessors.get('cat_low_imputer')\n",
    "    ohe_low = preprocessors.get('ohe_low')\n",
    "    cat_high_imputer = preprocessors.get('cat_high_imputer')\n",
    "    label_encoders = preprocessors.get('label_encoders', {})\n",
    "    vocab_sizes = preprocessors.get('vocab_sizes', {})\n",
    "    unk_token = '<UNK>'\n",
    "\n",
    "    # --- Numerical Features ---\n",
    "    fitted_num_cols_for_modeltype = preprocessors.get(f'fitted_cols_num_{model_type_flag}', [])\n",
    "\n",
    "    cols_to_process_num = [col for col in num_feat_model_list if col in df_proc.columns and col in fitted_num_cols_for_modeltype]\n",
    "\n",
    "    if cols_to_process_num and num_imputer and scaler:\n",
    "        X_num_imputed = num_imputer.transform(df_proc[cols_to_process_num])\n",
    "        X_num_scaled = scaler.transform(X_num_imputed)\n",
    "    else:\n",
    "        expected_shape_num = len(fitted_num_cols_for_modeltype) if scaler else 0\n",
    "        X_num_scaled = np.zeros((len(df_proc), expected_shape_num))\n",
    "\n",
    "    fitted_cat_low_cols = preprocessors.get('fitted_cols_cat_low', [])\n",
    "    cols_to_process_cat_low = [col for col in cat_low_feat_model_list if col in df_proc.columns and col in fitted_cat_low_cols]\n",
    "\n",
    "    if cols_to_process_cat_low and cat_low_imputer and ohe_low:\n",
    "        X_cat_low_imputed = cat_low_imputer.transform(df_proc[cols_to_process_cat_low])\n",
    "        X_cat_low_ohe = ohe_low.transform(X_cat_low_imputed)\n",
    "    else:\n",
    "        expected_shape_ohe = len(ohe_low.get_feature_names_out()) if ohe_low and hasattr(ohe_low, 'get_feature_names_out') else 0\n",
    "        X_cat_low_ohe = np.zeros((len(df_proc), expected_shape_ohe))\n",
    "\n",
    "    X_main_input = np.hstack([X_num_scaled, X_cat_low_ohe])\n",
    "\n",
    "    keras_input_list = [X_main_input]\n",
    "    expected_high_card_keras_inputs = ['HomeTeam', 'AwayTeam', 'Division']\n",
    "\n",
    "    fitted_cat_high_cols = preprocessors.get('fitted_cols_cat_high', [])\n",
    "    if cat_high_imputer and any(col in fitted_cat_high_cols for col in cat_high_feat_model_list):\n",
    "        cols_to_impute_high_cat = [col for col in cat_high_feat_model_list if col in df_proc.columns and col in fitted_cat_high_cols]\n",
    "        if cols_to_impute_high_cat:\n",
    "            df_proc[cols_to_impute_high_cat] = cat_high_imputer.transform(df_proc[cols_to_impute_high_cat])\n",
    "\n",
    "    for col_name in expected_high_card_keras_inputs:\n",
    "        le_key = 'Team' if 'Team' in col_name else 'Division'\n",
    "        le = label_encoders.get(le_key)\n",
    "\n",
    "        if le is None: raise ValueError(f\"LabelEncoder '{le_key}' for '{col_name}' not found.\")\n",
    "\n",
    "        effective_vocab_size = vocab_sizes.get(le_key, len(le.classes_))\n",
    "        try:\n",
    "            if loaded_model_ref:\n",
    "                embedding_layer = loaded_model_ref.get_layer(name=f'embedding_{le_key.lower()}')\n",
    "                effective_vocab_size = embedding_layer.input_dim\n",
    "        except: pass\n",
    "\n",
    "        if col_name in df_proc.columns:\n",
    "            encoded_col = df_proc[col_name].astype(str).apply(\n",
    "                lambda x: le.transform([x])[0] if x in le.classes_ else le.transform([unk_token])[0]\n",
    "            )\n",
    "            encoded_col_capped = encoded_col.apply(lambda idx: min(idx, effective_vocab_size - 1))\n",
    "            keras_input_list.append(encoded_col_capped.values.reshape(-1, 1))\n",
    "        else: \n",
    "            unk_idx_capped = min(le.transform([unk_token])[0], effective_vocab_size - 1)\n",
    "            keras_input_list.append(np.full((len(df_proc), 1), unk_idx_capped))\n",
    "\n",
    "    return keras_input_list\n",
    "\n",
    "\n",
    "# --- Main Data Processing Flow ---\n",
    "print(\"--- Starting Data Preparation ---\")\n",
    "df_full_dataset = load_and_clean_data(DATA_FILE, FEATURES_TO_LOAD_FROM_CSV, TARGET_COL, ODDS_COLS)\n",
    "df_engineered_full = engineer_features(df_full_dataset)\n",
    "del df_full_dataset; gc.collect()\n",
    "\n",
    "preprocessors = fit_preprocessors(\n",
    "    df_engineered_full,\n",
    "    NUMERICAL_FEATURES_FULL_MODEL,\n",
    "    NUMERICAL_FEATURES_REDUCED_MODEL,\n",
    "    CATEGORICAL_LOW_CARDINALITY_MODEL,\n",
    "    CATEGORICAL_HIGH_CARDINALITY_MODEL\n",
    ")\n",
    "\n",
    "df_latest_raw_engineered = df_engineered_full.iloc[-EVALUATION_SUBSET_SIZE:].copy()\n",
    "del df_engineered_full; gc.collect()\n",
    "\n",
    "actual_raw_requirements_for_full_model = [\n",
    "    col for col in FULL_MODEL_RAW_DATA_REQUIREMENTS if col in df_latest_raw_engineered.columns\n",
    "]\n",
    "\n",
    "if actual_raw_requirements_for_full_model:\n",
    "    df_evaluation_subset = df_latest_raw_engineered.dropna(subset=actual_raw_requirements_for_full_model).copy()\n",
    "else:\n",
    "    print(f\"Warning: Key raw features for filtering evaluation subset ({FULL_MODEL_RAW_DATA_REQUIREMENTS}) not found in the engineered latest data. Using all {len(df_latest_raw_engineered)} latest matches.\")\n",
    "    df_evaluation_subset = df_latest_raw_engineered.copy()\n",
    "\n",
    "print(f\"Selected {len(df_evaluation_subset)} matches for final evaluation (after filtering for Full NN raw feature requirements).\")\n",
    "\n",
    "if df_evaluation_subset.empty:\n",
    "    raise SystemExit(\"Evaluation subset is empty after filtering. Cannot proceed.\")\n",
    "\n",
    "y_true_eval = df_evaluation_subset[TARGET_COL].map(TARGET_MAP).values\n",
    "print(f\"Target (y_true_eval) shape: {y_true_eval.shape}\")\n",
    "print(\"--- Data Preparation Complete ---\")"
   ],
   "id": "e2345d67f2f27c86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Preparation ---\n",
      "Selected 499 matches for final evaluation (after filtering for Full NN raw feature requirements).\n",
      "Target (y_true_eval) shape: (499,)\n",
      "--- Data Preparation Complete ---\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model prediction and evaluation\n",
    "\n",
    "asdasda\n"
   ],
   "id": "25e839d7d0d093e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T21:18:49.148533Z",
     "start_time": "2025-05-10T21:18:46.448075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model_name, model_path, df_eval_data, y_true,\n",
    "                   preproc, num_feat, cat_low_feat, cat_high_feat, model_type_flag):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file not found: {model_path}. Skipping evaluation for {model_name}.\")\n",
    "        return {\"Accuracy\": np.nan, \"F1 (Macro)\": np.nan, \"Evaluated Samples\": 0}\n",
    "\n",
    "    model = None\n",
    "    keras_eval_inputs = None\n",
    "\n",
    "    try:\n",
    "        model = load_model(model_path, compile=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_path}: {e}\")\n",
    "        return {\"Accuracy\": np.nan, \"F1 (Macro)\": np.nan, \"Evaluated Samples\": 0} \n",
    "\n",
    "    try:\n",
    "        keras_eval_inputs = transform_data_for_keras(\n",
    "            df_eval_data, preproc, num_feat, cat_low_feat, cat_high_feat, model_type_flag, model\n",
    "        )\n",
    "\n",
    "        pred_proba = model.predict(keras_eval_inputs, batch_size=NN_BATCH_SIZE, verbose=0)\n",
    "        preds = np.argmax(pred_proba, axis=1)\n",
    "\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        _, _, f1_macro, _ = precision_recall_fscore_support(y_true, preds, average='macro', zero_division=0)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"Accuracy\": acc,\n",
    "            \"F1 (Macro)\": f1_macro,\n",
    "            \"Evaluated Samples\": len(y_true)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation of {model_name}: {e}\")\n",
    "        return {\"Accuracy\": np.nan, \"F1 (Macro)\": np.nan,\n",
    "                \"Evaluated Samples\": len(y_true) if 'y_true' in locals() else 0}\n",
    "    finally:\n",
    "        if model is not None: del model\n",
    "        if keras_eval_inputs is not None: del keras_eval_inputs\n",
    "        gc.collect()\n",
    "        if tf.keras.backend.is_keras_tensor(tf.zeros(1)):\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# --- Run Evaluations ---\n",
    "results_summary = {}\n",
    "default_metrics = {\"Accuracy\": np.nan, \"F1 (Macro)\": np.nan, \"Evaluated Samples\": 0}\n",
    "\n",
    "if 'df_evaluation_subset' in locals() and not df_evaluation_subset.empty:\n",
    "    if os.path.exists(MODEL_FULL_PATH):\n",
    "        results_summary['Full Features NN'] = evaluate_model(\n",
    "            \"Full Features NN\", MODEL_FULL_PATH, df_evaluation_subset, y_true_eval,\n",
    "            preprocessors, NUMERICAL_FEATURES_FULL_MODEL, CATEGORICAL_LOW_CARDINALITY_MODEL, CATEGORICAL_HIGH_CARDINALITY_MODEL, 'full'\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Full Features Model not found at {MODEL_FULL_PATH}. Skipping.\")\n",
    "        results_summary['Full Features NN'] = default_metrics.copy()\n",
    "\n",
    "\n",
    "    if os.path.exists(MODEL_REDUCED_PATH):\n",
    "        results_summary['Reduced Features NN'] = evaluate_model(\n",
    "            \"Reduced Features NN\", MODEL_REDUCED_PATH, df_evaluation_subset, y_true_eval,\n",
    "            preprocessors, NUMERICAL_FEATURES_REDUCED_MODEL, CATEGORICAL_LOW_CARDINALITY_MODEL, CATEGORICAL_HIGH_CARDINALITY_MODEL, 'reduced'\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Reduced Features Model not found at {MODEL_REDUCED_PATH}. Skipping.\")\n",
    "        results_summary['Reduced Features NN'] = default_metrics.copy()\n",
    "else:\n",
    "    print(\"Evaluation subset is empty or not defined. Skipping model evaluations.\")\n",
    "    results_summary['Full Features NN'] = default_metrics.copy()\n",
    "    results_summary['Reduced Features NN'] = default_metrics.copy()\n",
    "\n",
    "\n",
    "# --- Final Summary Output ---\n",
    "print(\"\\n\\n--- Evaluation Summary ---\")\n",
    "if not results_summary:\n",
    "    print(\"No models were evaluated.\")\n",
    "else:\n",
    "    summary_df = pd.DataFrame.from_dict(results_summary, orient='index')\n",
    "\n",
    "    desired_cols_summary = [\n",
    "        \"Evaluated Samples\", \"Accuracy\", \"F1 (Macro)\"\n",
    "    ]\n",
    "    cols_to_display = [col for col in desired_cols_summary if col in summary_df.columns]\n",
    "\n",
    "    if cols_to_display: \n",
    "        summary_df_simplified = summary_df[cols_to_display]\n",
    "        if not summary_df_simplified.empty:\n",
    "            try:\n",
    "                print(summary_df_simplified.to_markdown(floatfmt=\".4f\"))\n",
    "            except ImportError:\n",
    "                print(summary_df_simplified.to_string(float_format=\"%.4f\"))\n",
    "        else:\n",
    "            print(\"Simplified summary is empty (no relevant columns found or data).\")\n",
    "    else:\n",
    "        print(\"No columns to display in the simplified summary.\")\n",
    "\n",
    "print(\"\\n--- Evaluation Script Finished ---\")"
   ],
   "id": "9efc564fa0001e3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "                     Evaluated Samples  Accuracy  F1 (Macro)\n",
      "Full Features NN                   499    0.5090      0.3823\n",
      "Reduced Features NN                499    0.4830      0.3341\n",
      "\n",
      "--- Evaluation Script Finished ---\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# B. Evaluating In-play model",
   "id": "7c50503e6c8f3886"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data loading\n",
    "\n",
    "asdasd"
   ],
   "id": "9375d0c892239310"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T21:22:20.842563Z",
     "start_time": "2025-05-10T21:22:20.812557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "from scipy.stats import entropy\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, mean_absolute_error, mean_squared_error,\n",
    "                             brier_score_loss, r2_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (accuracy_score, mean_absolute_error, mean_squared_error,\n",
    "                             r2_score, roc_auc_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "pd.options.mode.chained_assignment = None \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "DATA_FILE = 'training/football_data.csv' \n",
    "FINAL_MODEL_DIR = 'models/in_play/'     \n",
    "\n",
    "CLUSTER_COLS = ['C_LTH', 'C_HTB', 'C_LTA', 'C_VAD', 'C_VHD', 'C_PHB'] \n",
    "\n",
    "EVALUATION_SUBSET_NAME = \"Latest 500\"\n",
    "EVALUATION_SUBSET_SIZE = 500"
   ],
   "id": "90872897a8f644cb",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data cleaning, pre-processing and feature engineering\n",
    "\n",
    "sadasda"
   ],
   "id": "afd77aa82be67c7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T21:22:29.846552Z",
     "start_time": "2025-05-10T21:22:23.122303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pipeline_prerequisites_from_inplay_dir(model_dir_path):\n",
    "    components = {}\n",
    "    files_to_load_map = {\n",
    "        'scaler_inplay': 'scaler.pkl',\n",
    "        'feature_names': 'feature_names.pkl',\n",
    "        'target_transformer_nn': 'target_transformer.pkl',\n",
    "        'iso_reg_calibrator': 'draw_calibrator.pkl',\n",
    "        'label_encoder': 'label_encoder.pkl', \n",
    "        'meta_model_final': 'meta_model.pkl',\n",
    "        'cluster_medians': 'cluster_medians.pkl',\n",
    "        'meta_feature_names': 'meta_feature_names.pkl',\n",
    "        'base_nn_home': 'base_nn_home.pkl', 'base_nn_away': 'base_nn_away.pkl',\n",
    "        'base_xgb_home': 'base_xgb_home.pkl', 'base_xgb_away': 'base_xgb_away.pkl',\n",
    "        'base_lgbm_home': 'base_lgbm_home.pkl', 'base_lgbm_away': 'base_lgbm_away.pkl',\n",
    "        'base_draw_specialist': 'base_draw_specialist.pkl',\n",
    "        'base_hda_classifier': 'base_hda_classifier.pkl'\n",
    "    }\n",
    "    all_successful = True\n",
    "    for key, fname in files_to_load_map.items():\n",
    "        fpath = os.path.join(model_dir_path, fname)\n",
    "        try:\n",
    "            with open(fpath, 'rb') as f: components[key] = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            if key == 'cluster_medians': components[key] = None\n",
    "            else: all_successful = False; print(f\"   ERROR: Required file not found: {fpath}\")\n",
    "        except Exception as e: all_successful = False; print(f\"   ERROR loading {fpath}: {e}\")\n",
    "\n",
    "    if not all_successful: return None\n",
    "    if 'feature_names' in components and components['feature_names'] is not None and not isinstance(components['feature_names'], list):\n",
    "        raise TypeError(\"Loaded 'feature_names' is not a list.\")\n",
    "    return components\n",
    "\n",
    "# --- Data Loading and Extensive Feature Engineering ---\n",
    "def load_and_engineer_data_like_training(data_file_path, cluster_cols_config):\n",
    "    try:\n",
    "        df = pd.read_csv(data_file_path, low_memory=False)\n",
    "    except FileNotFoundError: raise\n",
    "    except Exception as e: raise\n",
    "\n",
    "    initial_rows = len(df)\n",
    "    required_cols = ['FTHome', 'FTAway', 'OddHome', 'OddDraw', 'OddAway', 'HomeElo', 'AwayElo']\n",
    "    missing_required = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_required: raise ValueError(f\"Missing essential columns for cleaning: {missing_required}\")\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    df = df[(df['OddHome'] >= 1.0) & (df['OddDraw'] >= 1.0) & (df['OddAway'] >= 1.0)]\n",
    "    if 'Form3Home' in df.columns: df = df.dropna(subset=['Form3Home'])\n",
    "\n",
    "    essential_stats_cols = ['HomeShots', 'AwayShots', 'HomeTarget', 'AwayTarget', 'HomeCorners', 'AwayCorners']\n",
    "    existing_stats_cols = [col for col in essential_stats_cols if col in df.columns] \n",
    "    if existing_stats_cols: \n",
    "        cols_to_dropna_stats = [col for col in essential_stats_cols if col in df.columns]\n",
    "        if cols_to_dropna_stats: df = df.dropna(subset=cols_to_dropna_stats)\n",
    "\n",
    "    actual_cluster_cols_present = [col for col in cluster_cols_config if col in df.columns]\n",
    "    if actual_cluster_cols_present:\n",
    "        df = df.dropna(subset=actual_cluster_cols_present)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    if len(df) == 0: raise ValueError(\"No data remaining after initial cleaning.\")\n",
    "\n",
    "    def combine_odds_fe(odd1, odd2):\n",
    "        if pd.isna(odd1) or pd.isna(odd2) or odd1 < 1.0 or odd2 < 1.0: return np.nan\n",
    "        prob1, prob2 = 1.0 / odd1, 1.0 / odd2; combined_prob = prob1 + prob2;\n",
    "        return max(1.0 / combined_prob, 1.00) if combined_prob > 0 else np.nan\n",
    "\n",
    "    if 'MatchDate' in df.columns: df['MatchDate'] = pd.to_datetime(df['MatchDate'], errors='coerce')\n",
    "    else: df['MatchDate'] = pd.NaT\n",
    "    if 'MatchTime' in df.columns:\n",
    "        df['MatchTime'] = df['MatchTime'].astype(str).fillna('00:00:00').str.replace('.', ':', regex=False)\n",
    "        try: df['MatchTime'] = pd.to_datetime(df['MatchTime'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "        except Exception: df['MatchTime'] = pd.to_datetime('00:00:00', errors='coerce').dt.time\n",
    "        df['MatchTime'] = df['MatchTime'].fillna(pd.to_datetime('00:00:00').time())\n",
    "    else: df['MatchTime'] = pd.to_datetime('00:00:00').time()\n",
    "\n",
    "    valid_dt_idx = df['MatchDate'].notna() & df['MatchTime'].notna()\n",
    "    df['MatchDateTime'] = pd.NaT\n",
    "    if valid_dt_idx.any():\n",
    "        df.loc[valid_dt_idx, 'MatchDateTime'] = pd.to_datetime(df.loc[valid_dt_idx, 'MatchDate'].astype(str) + ' ' + df.loc[valid_dt_idx, 'MatchTime'].astype(str), errors='coerce')\n",
    "\n",
    "    if 'HTHome' in df.columns and 'HTAway' in df.columns:\n",
    "        df['HTHome'] = pd.to_numeric(df['HTHome'], errors='coerce'); df['HTAway'] = pd.to_numeric(df['HTAway'], errors='coerce')\n",
    "        df['HTTotalGoals'] = df['HTHome'].fillna(0) + df['HTAway'].fillna(0) \n",
    "\n",
    "    df['1XOdd'] = df.apply(lambda x: combine_odds_fe(x.get('OddHome'), x.get('OddDraw')), axis=1)\n",
    "    df['X2Odd'] = df.apply(lambda x: combine_odds_fe(x.get('OddDraw'), x.get('OddAway')), axis=1)\n",
    "    df['12Odd'] = df.apply(lambda x: combine_odds_fe(x.get('OddHome'), x.get('OddAway')), axis=1)\n",
    "    if all(c in df.columns for c in ['MaxHome', 'MaxDraw', 'MaxAway']):\n",
    "        df['Max1XOdd'] = df.apply(lambda x: combine_odds_fe(x['MaxHome'], x['MaxDraw']), axis=1)\n",
    "        df['MaxX2Odd'] = df.apply(lambda x: combine_odds_fe(x['MaxDraw'], x['MaxAway']), axis=1)\n",
    "        df['Max12Odd'] = df.apply(lambda x: combine_odds_fe(x['MaxHome'], x['MaxAway']), axis=1)\n",
    "\n",
    "    if 'HomeElo' in df.columns and 'AwayElo' in df.columns:\n",
    "        df['HomeElo'] = pd.to_numeric(df['HomeElo'], errors='coerce'); df['AwayElo'] = pd.to_numeric(df['AwayElo'], errors='coerce')\n",
    "        home_elo_median = df['HomeElo'].median(); away_elo_median = df['AwayElo'].median()\n",
    "        df['HomeElo'].fillna(home_elo_median if pd.notna(home_elo_median) else 1500.0, inplace=True)\n",
    "        df['AwayElo'].fillna(away_elo_median if pd.notna(away_elo_median) else 1500.0, inplace=True)\n",
    "        df['EloDifference'] = df['HomeElo'] - df['AwayElo']\n",
    "        df['EloTotal'] = df['HomeElo'] + df['AwayElo']\n",
    "        df['EloAdvantage'] = np.divide(df['EloDifference'], df['EloTotal'], out=np.zeros_like(df['EloDifference'], dtype=float), where=df['EloTotal']!=0)\n",
    "\n",
    "    form_cols_src_fe = ['Form3Home', 'Form5Home', 'Form3Away', 'Form5Away']\n",
    "    for col in form_cols_src_fe:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            median_val = df[col].median() \n",
    "            df[col].fillna(median_val if pd.notna(median_val) else 0, inplace=True)\n",
    "        else: df[col] = 0 \n",
    "    if all(c in df.columns for c in form_cols_src_fe):\n",
    "        df['Form3Difference'] = df['Form3Home'] - df['Form3Away']\n",
    "        df['Form5Difference'] = df['Form5Home'] - df['Form5Away']\n",
    "        df['FormOlderHome'] = df['Form5Home'] - df['Form3Home']\n",
    "        df['FormOlderAway'] = df['Form5Away'] - df['Form3Away']\n",
    "        df['FormMomentumHome'] = df['Form3Home'] - df['FormOlderHome']\n",
    "        df['FormMomentumAway'] = df['Form3Away'] - df['FormOlderAway']\n",
    "        df['FormMomentumDiff'] = df['FormMomentumHome'] - df['FormMomentumAway']\n",
    "\n",
    "    df['OddsDifference'] = df['OddHome'] - df['OddAway']\n",
    "    if 'MaxHome' in df.columns and 'MaxAway' in df.columns:\n",
    "        df['MaxOddsDifference'] = df['MaxHome'] - df['MaxAway']\n",
    "\n",
    "    df['ImpliedProbHome'] = 1.0 / df['OddHome']; df['ImpliedProbDraw'] = 1.0 / df['OddDraw']; df['ImpliedProbAway'] = 1.0 / df['OddAway']\n",
    "    df['ImpliedProbTotal'] = df['ImpliedProbHome'] + df['ImpliedProbDraw'] + df['ImpliedProbAway']\n",
    "    df['BookmakerMargin'] = df['ImpliedProbTotal'] - 1.0\n",
    "\n",
    "    stat_cols_fe = ['HomeShots', 'AwayShots', 'HomeTarget', 'AwayTarget'] \n",
    "    stat_features_created_fe = all(c in df.columns for c in stat_cols_fe)\n",
    "    if stat_features_created_fe:\n",
    "        for col in stat_cols_fe: \n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce'); df[col] = df[col].fillna(df[col].median())\n",
    "        df['ShotsDifference'] = df['HomeShots'] - df['AwayShots']\n",
    "        df['ShotsTotal'] = df['HomeShots'] + df['AwayShots']\n",
    "        df['TargetDifference'] = df['HomeTarget'] - df['AwayTarget']\n",
    "        df['TargetTotal'] = df['HomeTarget'] + df['AwayTarget']\n",
    "        df['ShotAccuracyHome'] = np.divide(df['HomeTarget'], df['HomeShots'], out=np.zeros_like(df['HomeTarget'], dtype=float), where=df['HomeShots']!=0)\n",
    "        df['ShotAccuracyAway'] = np.divide(df['AwayTarget'], df['AwayShots'], out=np.zeros_like(df['AwayTarget'], dtype=float), where=df['AwayShots']!=0)\n",
    "        df['ShotAccuracyDiff'] = df['ShotAccuracyHome'] - df['ShotAccuracyAway']\n",
    "\n",
    "    corner_cols_fe = ['HomeCorners', 'AwayCorners']\n",
    "    corner_features_created_fe = all(c in df.columns for c in corner_cols_fe)\n",
    "    if corner_features_created_fe:\n",
    "        for col in corner_cols_fe: df[col] = pd.to_numeric(df[col], errors='coerce'); df[col] = df[col].fillna(df[col].median())\n",
    "        df['CornersDifference'] = df['HomeCorners'] - df['AwayCorners']\n",
    "        df['CornersTotal'] = df['HomeCorners'] + df['AwayCorners']\n",
    "        if stat_features_created_fe and 'ShotsDifference' in df.columns: \n",
    "            df['GameDominanceIndex'] = (df.get('ShotsDifference',0).fillna(0) + df.get('CornersDifference',0).fillna(0)) / 2.0\n",
    "        elif 'CornersDifference' in df.columns: df['GameDominanceIndex'] = df['CornersDifference'].fillna(0)\n",
    "        else: df['GameDominanceIndex'] = 0\n",
    "    elif 'GameDominanceIndex' in (model_expected_feature_names or []): df['GameDominanceIndex'] = 0 \n",
    "\n",
    "    disc_cols_fe = ['HomeYellow', 'AwayYellow', 'HomeRed', 'AwayRed', 'HomeFouls', 'AwayFouls']\n",
    "    discipline_features_created_fe = all(c in df.columns for c in disc_cols_fe)\n",
    "    if discipline_features_created_fe:\n",
    "        for col in disc_cols_fe: df[col] = pd.to_numeric(df[col], errors='coerce'); df[col] = df[col].fillna(df[col].median())\n",
    "        df['CardPointsHome'] = df['HomeYellow'] + (2 * df['HomeRed'])\n",
    "        df['CardPointsAway'] = df['AwayYellow'] + (2 * df['AwayRed'])\n",
    "        df['CardPointsDiff'] = df['CardPointsHome'] - df['CardPointsAway']\n",
    "        df['FoulsDifference'] = df['HomeFouls'] - df['AwayFouls']\n",
    "        df['FoulsTotal'] = df['HomeFouls'] + df['AwayFouls']\n",
    "\n",
    "    adv_deps_ots_fe = ['HomeTarget','HomeShots','Form5Home','ImpliedProbHome','AwayTarget','AwayShots','Form5Away','ImpliedProbAway','HomeFouls','HomeElo','AwayShots','AwayTarget','AwayFouls','AwayElo','EloDifference','ImpliedProbDraw','Form5Difference','FormMomentumHome','ShotAccuracyHome','FormMomentumAway','ShotAccuracyAway','Under25','HomeCorners','AwayCorners']\n",
    "    existing_deps_fe = [dep for dep in adv_deps_ots_fe if dep in df.columns]\n",
    "    advanced_features_possible_fe = len(existing_deps_fe) == len(adv_deps_ots_fe)\n",
    "\n",
    "    if advanced_features_possible_fe:\n",
    "        for col in existing_deps_fe:\n",
    "            if df[col].isnull().any():\n",
    "                median_val = df[col].median()\n",
    "                df[col].fillna(median_val if pd.notna(median_val) else 0, inplace=True)\n",
    "\n",
    "        df['ScoringEfficiencyHome']=(df['HomeTarget']/(df['HomeShots'].clip(lower=1)))*df['Form5Home']*df['ImpliedProbHome'] \n",
    "        df['ScoringEfficiencyAway']=(df['AwayTarget']/(df['AwayShots'].clip(lower=1)))*df['Form5Away']*df['ImpliedProbAway'] \n",
    "        df['DefensiveRatingHome']=np.divide(df['HomeFouls']*df['HomeElo'],(df['AwayShots']+df['AwayTarget']).clip(lower=1),out=np.zeros_like(df['HomeFouls'],dtype=float),where=(df['AwayShots']+df['AwayTarget']).clip(lower=1)!=0)\n",
    "        df['DefensiveRatingAway']=np.divide(df['AwayFouls']*df['AwayElo'],(df['HomeShots']+df['HomeTarget']).clip(lower=1),out=np.zeros_like(df['AwayFouls'],dtype=float),where=(df['HomeShots']+df['HomeTarget']).clip(lower=1)!=0)\n",
    "        df['DrawLikelihood']=(1-abs(df['EloDifference'])/1000).clip(lower=0)*df['ImpliedProbDraw']*(1-abs(df['Form5Difference'])/15).clip(lower=0)\n",
    "        df['FormEfficiencyHome']=df['Form5Home']*df.get('FormMomentumHome',0)*df.get('ShotAccuracyHome',0) \n",
    "        df['FormEfficiencyAway']=df['Form5Away']*df.get('FormMomentumAway',0)*df.get('ShotAccuracyAway',0) \n",
    "        df['CleanSheetProbHome']=1/(1+np.exp(-(df.get('DefensiveRatingHome',0)-df.get('ScoringEfficiencyAway',0))))\n",
    "        df['CleanSheetProbAway']=1/(1+np.exp(-(df.get('DefensiveRatingAway',0)-df.get('ScoringEfficiencyHome',0))))\n",
    "        under25_median_ots = df.get('Under25', pd.Series(dtype='float')).median() if 'Under25' in df and not df['Under25'].isnull().all() else 0.5\n",
    "        df['LowScoreIndicator']=df.get('Under25', under25_median_ots)*df.get('ImpliedProbDraw',0)*(1/(df.get('HomeTarget',0)+df.get('AwayTarget',0)+1))\n",
    "        df['DrawTendency']=((1-abs(df.get('EloDifference',0))/1000).clip(lower=0)*df.get('ImpliedProbDraw',0)*(1-abs(df.get('Form5Difference',0))/15).clip(lower=0)*(1/(abs(df.get('HomeTarget',0)-df.get('AwayTarget',0))+1)))\n",
    "        df['DefensiveOrganization']=np.divide((df.get('HomeFouls',0)+df.get('AwayFouls',0)), (df.get('HomeShots',0)+df.get('AwayShots',0)+1).clip(lower=1), out=np.zeros_like(df.get('HomeFouls',pd.Series(0,index=df.index)),dtype=float), where=(df.get('HomeShots',0)+df.get('AwayShots',0)+1).clip(lower=1)!=0)*df.get('Under25', under25_median_ots)\n",
    "        df['HomeDefensiveStyle']=np.divide(df.get('HomeFouls',0)*df.get('HomeCorners',0), (df.get('HomeShots',0)+1).clip(lower=1), out=np.zeros_like(df.get('HomeFouls',pd.Series(0,index=df.index)),dtype=float), where=(df.get('HomeShots',0)+1).clip(lower=1)!=0)\n",
    "        df['AwayDefensiveStyle']=np.divide(df.get('AwayFouls',0)*df.get('AwayCorners',0), (df.get('AwayShots',0)+1).clip(lower=1), out=np.zeros_like(df.get('AwayFouls',pd.Series(0,index=df.index)),dtype=float), where=(df.get('AwayShots',0)+1).clip(lower=1)!=0)\n",
    "        df['ExpectedGoalsHome']=df.get('HomeTarget',0)*df.get('ShotAccuracyHome',0)*(df.get('Form5Home',0)/15).clip(0,1)*df.get('ImpliedProbHome',0)\n",
    "        df['ExpectedGoalsAway']=df.get('AwayTarget',0)*df.get('ShotAccuracyAway',0)*(df.get('Form5Away',0)/15).clip(0,1)*df.get('ImpliedProbAway',0)\n",
    "\n",
    "    df.sort_values(by='MatchDateTime', ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "# --- Main Data Preparation Flow ---\n",
    "pipeline_components = load_pipeline_prerequisites_from_inplay_dir(FINAL_MODEL_DIR)\n",
    "if not pipeline_components:\n",
    "    raise SystemExit(\"Failed to load model pipeline components. Exiting.\")\n",
    "\n",
    "loaded_feature_names_from_pkl = pipeline_components.get('feature_names')\n",
    "if not loaded_feature_names_from_pkl:\n",
    "    raise SystemExit(\"Error: 'feature_names.pkl' not loaded or empty from pipeline components.\")\n",
    "\n",
    "df_full_engineered = load_and_engineer_data_like_training(DATA_FILE, CLUSTER_COLS)\n",
    "\n",
    "for feature_col_name in loaded_feature_names_from_pkl:\n",
    "    if feature_col_name not in df_full_engineered.columns:\n",
    "        print(f\"Final Alignment Warning: Feature '{feature_col_name}' (from feature_names.pkl) missing after FE. Adding with 0s.\")\n",
    "        df_full_engineered[feature_col_name] = 0\n",
    "\n",
    "X_to_scale = df_full_engineered[loaded_feature_names_from_pkl].copy()\n",
    "\n",
    "if X_to_scale.isnull().sum().sum() > 0:\n",
    "    for col in X_to_scale.columns[X_to_scale.isnull().any()]:\n",
    "        median_val = X_to_scale[col].median()\n",
    "        X_to_scale[col].fillna(median_val if pd.notna(median_val) else 0, inplace=True)\n",
    "    if X_to_scale.isnull().sum().sum() > 0: X_to_scale.fillna(0, inplace=True)\n",
    "\n",
    "scaler = pipeline_components.get('scaler_inplay')\n",
    "if not scaler: raise SystemExit(\"Error: Scaler ('scaler.pkl') not loaded.\")\n",
    "try:\n",
    "    X_full_scaled_values = scaler.transform(X_to_scale)\n",
    "    X_full_scaled_df = pd.DataFrame(X_full_scaled_values, columns=loaded_feature_names_from_pkl, index=df_full_engineered.index)\n",
    "except ValueError as e:\n",
    "    print(f\"Error during scaling: {e}\")\n",
    "    raise SystemExit(\"Halting due to scaling error.\")\n",
    "\n",
    "if len(df_full_engineered) < EVALUATION_SUBSET_SIZE:\n",
    "    df_evaluation_subset_orig = df_full_engineered.loc[X_full_scaled_df.index].copy()\n",
    "    X_evaluation_subset_scaled = X_full_scaled_df.copy()\n",
    "else:\n",
    "    latest_indices = df_full_engineered.index[-EVALUATION_SUBSET_SIZE:]\n",
    "    df_evaluation_subset_orig = df_full_engineered.loc[latest_indices].copy()\n",
    "    X_evaluation_subset_scaled = X_full_scaled_df.loc[latest_indices].copy()\n",
    "\n",
    "print(f\"Evaluation subset selected: {len(df_evaluation_subset_orig)} matches.\")\n",
    "\n",
    "y_true_scores_eval = df_evaluation_subset_orig[['FTHome', 'FTAway']].copy()\n",
    "y_true_hda_eval_text = ['H' if h > a else ('A' if h < a else 'D') for h, a in y_true_scores_eval.values]\n",
    "\n",
    "le_outcome = pipeline_components.get('label_encoder')\n",
    "if not le_outcome: raise SystemExit(\"Error: LabelEncoder not loaded.\")\n",
    "try:\n",
    "    y_true_hda_eval_numeric = le_outcome.transform(y_true_hda_eval_text)\n",
    "    pipeline_components['le_classes_for_metrics'] = le_outcome.classes_\n",
    "except ValueError:\n",
    "    unique_labels_in_subset = sorted(list(set(y_true_hda_eval_text)))\n",
    "    temp_le = LabelEncoder().fit(unique_labels_in_subset)\n",
    "    y_true_hda_eval_numeric = temp_le.transform(y_true_hda_eval_text)\n",
    "    pipeline_components['label_encoder'] = temp_le \n",
    "    pipeline_components['le_classes_for_metrics'] = temp_le.classes_ \n",
    "\n",
    "print(\"--- Data Preparation for In-Play Model Complete ---\")"
   ],
   "id": "2f9e21001347755b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation subset selected: 500 matches.\n",
      "--- Data Preparation for In-Play Model Complete ---\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model prediction and evaluation\n",
    "\n",
    "\n",
    "sadadsada"
   ],
   "id": "d55afa36da5ea39e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T21:25:06.437247Z",
     "start_time": "2025-05-10T21:25:06.345761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_with_full_pipeline(X_scaled_subset, df_original_subset_for_clusters, components):\n",
    "    X_scaled_np = X_scaled_subset.values\n",
    "\n",
    "    preds_nn_home = components['base_nn_home'].predict(X_scaled_np)\n",
    "    preds_nn_away = components['base_nn_away'].predict(X_scaled_np)\n",
    "    preds_xgb_home = components['base_xgb_home'].predict(X_scaled_np)\n",
    "    preds_xgb_away = components['base_xgb_away'].predict(X_scaled_np)\n",
    "    preds_lgbm_home = components['base_lgbm_home'].predict(X_scaled_np)\n",
    "    preds_lgbm_away = components['base_lgbm_away'].predict(X_scaled_np)\n",
    "    preds_draw_prob = components['base_draw_specialist'].predict_proba(X_scaled_np)[:, 1]\n",
    "    preds_hda_probs_base = components['base_hda_classifier'].predict_proba(X_scaled_np)\n",
    "\n",
    "    if components.get('target_transformer_nn') is not None:\n",
    "        preds_nn_combined = np.column_stack([preds_nn_home, preds_nn_away])\n",
    "        finite_median_nn = 0.0\n",
    "        if np.any(np.isfinite(preds_nn_combined)):\n",
    "            finite_median_nn = np.nanmedian(preds_nn_combined[np.isfinite(preds_nn_combined)])\n",
    "\n",
    "        preds_nn_combined_clean = np.nan_to_num(\n",
    "            preds_nn_combined, nan=finite_median_nn,\n",
    "            posinf=(np.nanmax(preds_nn_combined[np.isfinite(preds_nn_combined)]) if np.any(np.isfinite(preds_nn_combined)) else finite_median_nn),\n",
    "            neginf=(np.nanmin(preds_nn_combined[np.isfinite(preds_nn_combined)]) if np.any(np.isfinite(preds_nn_combined)) else finite_median_nn)\n",
    "        )\n",
    "        if np.any(np.isnan(preds_nn_combined_clean)): preds_nn_combined_clean = np.nan_to_num(preds_nn_combined_clean, nan=0.0)\n",
    "\n",
    "        preds_nn_inv = components['target_transformer_nn'].inverse_transform(preds_nn_combined_clean)\n",
    "        preds_nn_home, preds_nn_away = preds_nn_inv[:, 0], preds_nn_inv[:, 1]\n",
    "\n",
    "    finite_median_draw = 0.33\n",
    "    if np.any(np.isfinite(preds_draw_prob)):\n",
    "        finite_median_draw = np.nanmedian(preds_draw_prob[np.isfinite(preds_draw_prob)])\n",
    "    preds_draw_prob_filled = pd.Series(preds_draw_prob).fillna(finite_median_draw).values\n",
    "    if np.any(~np.isfinite(preds_draw_prob_filled)):\n",
    "        preds_draw_prob_filled = np.nan_to_num(preds_draw_prob_filled, nan=0.33, posinf=1.0, neginf=0.0)\n",
    "\n",
    "    calibrator = components.get('iso_reg_calibrator')\n",
    "    if calibrator:\n",
    "        preds_draw_prob_filled_finite = np.nan_to_num(preds_draw_prob_filled, nan=0.33, posinf=1.0, neginf=0.0)\n",
    "        preds_draw_prob_calibrated = calibrator.transform(preds_draw_prob_filled_finite.reshape(-1, 1)).ravel()\n",
    "    else:\n",
    "        preds_draw_prob_calibrated = preds_draw_prob_filled\n",
    "    preds_draw_prob_calibrated = pd.Series(preds_draw_prob_calibrated, index=X_scaled_subset.index).clip(0.0, 1.0)\n",
    "\n",
    "    meta_features_df = pd.DataFrame(index=X_scaled_subset.index)\n",
    "    meta_features_df['draw_prob_calibrated'] = preds_draw_prob_calibrated\n",
    "    meta_features_df['nn_oof_home'] = preds_nn_home; meta_features_df['nn_oof_away'] = preds_nn_away\n",
    "    meta_features_df['xgb_oof_home'] = preds_xgb_home; meta_features_df['xgb_oof_away'] = preds_xgb_away\n",
    "    meta_features_df['lgbm_oof_home'] = preds_lgbm_home; meta_features_df['lgbm_oof_away'] = preds_lgbm_away\n",
    "    meta_features_df['nn_score_diff'] = meta_features_df['nn_oof_home'] - meta_features_df['nn_oof_away']\n",
    "    meta_features_df['xgb_score_diff'] = meta_features_df['xgb_oof_home'] - meta_features_df['xgb_oof_away']\n",
    "    meta_features_df['lgbm_score_diff'] = meta_features_df['lgbm_oof_home'] - meta_features_df['lgbm_oof_away']\n",
    "    meta_features_df['nn_abs_diff'] = meta_features_df['nn_score_diff'].abs()\n",
    "    meta_features_df['xgb_abs_diff'] = meta_features_df['xgb_score_diff'].abs()\n",
    "    meta_features_df['lgbm_abs_diff'] = meta_features_df['lgbm_score_diff'].abs()\n",
    "    meta_features_df['hda_predicted_class'] = np.argmax(preds_hda_probs_base, axis=1)\n",
    "    sorted_hda_probs = np.sort(preds_hda_probs_base, axis=1)\n",
    "    margin_calc = lambda row: row[-1] - row[-2] if len(row) >= 2 and pd.notna(row[-1]) and pd.notna(row[-2]) else 0.0\n",
    "    meta_features_df['hda_prob_margin'] = np.apply_along_axis(margin_calc, 1, sorted_hda_probs)\n",
    "    epsilon = 1e-9\n",
    "    safe_hda_probs = np.clip(preds_hda_probs_base, epsilon, 1.0 - epsilon)\n",
    "    safe_hda_probs = safe_hda_probs / np.sum(safe_hda_probs, axis=1, keepdims=True)\n",
    "    meta_features_df['hda_prob_entropy'] = entropy(safe_hda_probs, axis=1)\n",
    "\n",
    "    actual_cluster_cols_in_original_df = [col for col in CLUSTER_COLS if col in df_original_subset_for_clusters.columns]\n",
    "    if actual_cluster_cols_in_original_df:\n",
    "        cluster_features_subset = df_original_subset_for_clusters[actual_cluster_cols_in_original_df].loc[X_scaled_subset.index].copy()\n",
    "        if cluster_features_subset.isnull().sum().sum() > 0:\n",
    "            cluster_medians = components.get('cluster_medians')\n",
    "            cluster_features_subset = cluster_features_subset.fillna(cluster_medians if cluster_medians is not None else 0)\n",
    "        meta_features_df = pd.concat([meta_features_df, cluster_features_subset], axis=1)\n",
    "\n",
    "    meta_feature_names_expected = components.get('meta_feature_names')\n",
    "    if not meta_feature_names_expected:\n",
    "        meta_feature_names_expected = list(meta_features_df.columns)\n",
    "\n",
    "    for col in meta_feature_names_expected:\n",
    "        if col not in meta_features_df.columns: meta_features_df[col] = 0\n",
    "    meta_features_aligned = meta_features_df.reindex(columns=meta_feature_names_expected, fill_value=0)\n",
    "\n",
    "    if meta_features_aligned.isnull().sum().sum() > 0: meta_features_aligned.fillna(0, inplace=True)\n",
    "\n",
    "    meta_model = components.get('meta_model_final')\n",
    "    if not meta_model: raise ValueError(\"Meta-model not loaded.\")\n",
    "\n",
    "    final_preds_encoded = meta_model.predict(meta_features_aligned)\n",
    "    le_for_inverse_transform = components.get('label_encoder')\n",
    "    final_preds_hda = le_for_inverse_transform.inverse_transform(final_preds_encoded)\n",
    "    final_preds_probs = meta_model.predict_proba(meta_features_aligned)\n",
    "\n",
    "    results = {\n",
    "        \"pred_hda\": final_preds_hda, \"pred_probs\": final_preds_probs,\n",
    "        \"base_pred_nn_home\": meta_features_df.get('nn_oof_home', pd.Series(np.zeros(len(X_scaled_subset)), index=X_scaled_subset.index)).values,\n",
    "        \"base_pred_nn_away\": meta_features_df.get('nn_oof_away', pd.Series(np.zeros(len(X_scaled_subset)), index=X_scaled_subset.index)).values,\n",
    "        \"base_pred_xgb_home\": meta_features_df.get('xgb_oof_home', pd.Series(np.zeros(len(X_scaled_subset)), index=X_scaled_subset.index)).values,\n",
    "        \"base_pred_xgb_away\": meta_features_df.get('xgb_oof_away', pd.Series(np.zeros(len(X_scaled_subset)), index=X_scaled_subset.index)).values,\n",
    "        \"base_pred_lgbm_home\": meta_features_df.get('lgbm_oof_home', pd.Series(np.zeros(len(X_scaled_subset)), index=X_scaled_subset.index)).values,\n",
    "        \"base_pred_lgbm_away\": meta_features_df.get('lgbm_oof_away', pd.Series(np.zeros(len(X_scaled_subset)), index=X_scaled_subset.index)).values,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def calculate_and_format_metrics_ots_style(y_true_scores, y_true_hda_text_subset, predictions, le_for_eval):\n",
    "    \"\"\"Calculates metrics similar to the OTS script.\"\"\"\n",
    "    metrics_summary = {}\n",
    "    y_pred_hda_text = predictions[\"pred_hda\"]\n",
    "    y_pred_probs = predictions[\"pred_probs\"]\n",
    "\n",
    "    y_true_hda_numeric_subset = le_for_eval.transform(y_true_hda_text_subset)\n",
    "    y_pred_hda_numeric_subset = le_for_eval.transform(y_pred_hda_text)\n",
    "\n",
    "    metrics_summary[\"Outcome Accuracy\"] = accuracy_score(y_true_hda_numeric_subset, y_pred_hda_numeric_subset)\n",
    "\n",
    "    avg_base_pred_h = (predictions[\"base_pred_nn_home\"] + predictions[\"base_pred_xgb_home\"] + predictions[\"base_pred_lgbm_home\"]) / 3.0\n",
    "    avg_base_pred_a = (predictions[\"base_pred_nn_away\"] + predictions[\"base_pred_xgb_away\"] + predictions[\"base_pred_lgbm_away\"]) / 3.0\n",
    "    true_h_scores = y_true_scores['FTHome'].values; true_a_scores = y_true_scores['FTAway'].values\n",
    "\n",
    "    pred_h_round = np.round(avg_base_pred_h).astype(int)\n",
    "    pred_a_round = np.round(avg_base_pred_a).astype(int)\n",
    "    metrics_summary[\"Exact Score Acc\"] = np.mean((pred_h_round == true_h_scores) & (pred_a_round == true_a_scores))\n",
    "    metrics_summary[\"Exact Goal Diff\"] = np.mean((pred_h_round - pred_a_round) == (true_h_scores - true_a_scores))\n",
    "\n",
    "    metrics_summary[\"R2_Home\"] = r2_score(true_h_scores, avg_base_pred_h)\n",
    "    metrics_summary[\"R2_Away\"] = r2_score(true_a_scores, avg_base_pred_a)\n",
    "    metrics_summary[\"MSE_Home\"] = mean_squared_error(true_h_scores, avg_base_pred_h)\n",
    "    metrics_summary[\"MSE_Away\"] = mean_squared_error(true_a_scores, avg_base_pred_a)\n",
    "    metrics_summary[\"MAE_Home\"] = mean_absolute_error(true_h_scores, avg_base_pred_h)\n",
    "    metrics_summary[\"MAE_Away\"] = mean_absolute_error(true_a_scores, avg_base_pred_a)\n",
    "\n",
    "    roc_auc_w = np.nan\n",
    "    all_known_classes_string = le_for_eval.classes_\n",
    "    all_known_classes_numeric = le_for_eval.transform(all_known_classes_string)\n",
    "\n",
    "    if len(y_true_hda_numeric_subset) > 0 and y_pred_probs.shape[0] == len(y_true_hda_numeric_subset):\n",
    "        try:\n",
    "            y_true_bin = label_binarize(y_true_hda_numeric_subset, classes=all_known_classes_numeric)\n",
    "\n",
    "            if y_true_bin.shape[1] == y_pred_probs.shape[1]:\n",
    "                brier_m = np.mean(np.sum((y_pred_probs - y_true_bin)**2, axis=1))\n",
    "                if y_true_bin.shape[1] >= 2 and np.sum(np.any(y_true_bin, axis=0)) >= 2:\n",
    "                    if np.isnan(y_pred_probs).any():\n",
    "                        pass\n",
    "                    roc_auc_w = roc_auc_score(y_true_bin, y_pred_probs,\n",
    "                                              multi_class='ovr', average='weighted',\n",
    "                                              labels=all_known_classes_numeric) \n",
    "                pass\n",
    "        except ValueError as ve:\n",
    "            expected_errors = [\"Only one class present in y_true\",\n",
    "                               \"Number of classes in y_true not equal to the number of columns in 'y_score'\",\n",
    "                               \"Input 'y_true' does not contain all the labels defined in 'labels'\"]\n",
    "            if any(err_msg in str(ve) for err_msg in expected_errors):\n",
    "                pass\n",
    "            else:\n",
    "                pass \n",
    "        except Exception as e:\n",
    "            pass \n",
    "\n",
    "    metrics_summary[\"Brier Score\"] = brier_m\n",
    "    metrics_summary[\"ROC AUC (OvR W)\"] = roc_auc_w\n",
    "    metrics_summary[\"Evaluated Samples\"] = len(y_true_hda_numeric_subset)\n",
    "    return metrics_summary\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "evaluation_results_dict = {}\n",
    "default_metrics_inplay = {\n",
    "    \"Outcome Accuracy\": np.nan, \"Exact Score Acc\": np.nan, \"Exact Goal Diff\": np.nan,\n",
    "    \"R2_Home\": np.nan, \"R2_Away\": np.nan, \"MSE_Home\": np.nan, \"MSE_Away\": np.nan,\n",
    "    \"MAE_Home\": np.nan, \"MAE_Away\": np.nan, \"ROC AUC (OvR W)\": np.nan, \"Brier Score\": np.nan,\n",
    "    \"Evaluated Samples\": 0\n",
    "}\n",
    "\n",
    "if pipeline_components:\n",
    "    if 'X_evaluation_subset_scaled' in locals() and not X_evaluation_subset_scaled.empty and \\\n",
    "            'y_true_hda_eval_numeric' in locals() and len(y_true_hda_eval_numeric) > 0 and \\\n",
    "            'df_evaluation_subset_orig' in locals() and 'y_true_scores_eval' in locals():\n",
    "\n",
    "        predictions = predict_with_full_pipeline(X_evaluation_subset_scaled, df_evaluation_subset_orig, pipeline_components)\n",
    "\n",
    "        le_for_metrics_final = pipeline_components.get('label_encoder')\n",
    "\n",
    "        metrics = calculate_and_format_metrics_ots_style(y_true_scores_eval, y_true_hda_eval_text, predictions, le_for_metrics_final)\n",
    "        evaluation_results_dict[FINAL_MODEL_DIR.strip('/')] = metrics\n",
    "    else:\n",
    "        evaluation_results_dict[FINAL_MODEL_DIR.strip('/')] = default_metrics_inplay.copy()\n",
    "else:\n",
    "    evaluation_results_dict[FINAL_MODEL_DIR.strip('/')] = default_metrics_inplay.copy()\n",
    "\n",
    "print(\"\\n\\n--- In-Play Model Evaluation Summary ---\")\n",
    "if not evaluation_results_dict: print(\"No model was evaluated.\")\n",
    "else:\n",
    "    summary_df = pd.DataFrame.from_dict(evaluation_results_dict, orient='index')\n",
    "\n",
    "    model_name_key = FINAL_MODEL_DIR.strip('/')\n",
    "    if model_name_key in summary_df.index:\n",
    "        metrics_to_print = summary_df.loc[model_name_key]\n",
    "        print(f\"--- Metrics for: {EVALUATION_SUBSET_NAME} ({metrics_to_print.get('Evaluated Samples', 0):.0f} matches) ---\")\n",
    "        print(f\"   - Outcome Accuracy: {metrics_to_print.get('Outcome Accuracy', np.nan):.4f}\")\n",
    "        print(f\"   - Exact Score Acc:  {metrics_to_print.get('Exact Score Acc', np.nan):.4f}\")\n",
    "        print(f\"   - Exact Goal Diff:  {metrics_to_print.get('Exact Goal Diff', np.nan):.4f}\")\n",
    "        r2_h_val = metrics_to_print.get('R2_Home', np.nan); r2_a_val = metrics_to_print.get('R2_Away', np.nan)\n",
    "        print(f\"   - R2 (Home/Away):   {r2_h_val:.4f} / {r2_a_val:.4f}\")\n",
    "        mse_h_val = metrics_to_print.get('MSE_Home', np.nan); mse_a_val = metrics_to_print.get('MSE_Away', np.nan)\n",
    "        print(f\"   - MSE (Home/Away):  {mse_h_val:.4f} / {mse_a_val:.4f}\")\n",
    "        mae_h_val = metrics_to_print.get('MAE_Home', np.nan); mae_a_val = metrics_to_print.get('MAE_Away', np.nan)\n",
    "        print(f\"   - MAE (Home/Away):  {mae_h_val:.4f} / {mae_a_val:.4f}\")\n",
    "        print(f\"   - ROC AUC (OvR W):  {metrics_to_print.get('ROC AUC (OvR W)', np.nan):.4f}\")\n",
    "        print(f\"   - Brier Score Loss: {metrics_to_print.get('Brier Score', np.nan):.4f}\")\n",
    "    else:\n",
    "        print(f\"No results found for model '{model_name_key}'.\")\n",
    "\n",
    "print(\"\\n--- In-Play Evaluation Script Finished ---\")"
   ],
   "id": "47dae05977e9a0ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- In-Play Model Evaluation Summary ---\n",
      "--- Metrics for: Latest 500 (500 matches) ---\n",
      "   - Outcome Accuracy: 0.7140\n",
      "   - Exact Score Acc:  0.2980\n",
      "   - Exact Goal Diff:  0.3900\n",
      "   - R2 (Home/Away):   0.6468 / 0.6279\n",
      "   - MSE (Home/Away):  0.6059 / 0.4254\n",
      "   - MAE (Home/Away):  0.6213 / 0.5129\n",
      "   - ROC AUC (OvR W):  0.8756\n",
      "   - Brier Score Loss: 0.3882\n",
      "\n",
      "--- In-Play Evaluation Script Finished ---\n"
     ]
    }
   ],
   "execution_count": 139
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
